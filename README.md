# Luigi.AI

### Explanation:

This code utilizes Langchain and Streamlit to create a document-centric question answering application. Users can upload documents in various formats (PDF, docx, txt) and ask questions about their content. The application leverages OpenAI's GPT-3.5-turbo model to generate answers based on the uploaded document.

### Functionalities:

* The `load_document` function handles file upload and recognizes supported formats (PDF, docx, txt).
* The `chunk_data` function splits the document text into smaller chunks for efficient processing.

### Embedding Creation:

* The `create_embeddings` function utilizes Langchain's `OpenAIEmbeddings` class to generate vector representations (embeddings) for each text chunk.
* These embeddings capture the semantic meaning of the text and are stored in a Chroma vector store for fast retrieval.

### Question Answering:

* The `ask_and_get_answer` function takes the user's question and the vector store as inputs.
* It retrieves the `k` most relevant document chunks from the vector store based on their semantic similarity to the question.
* These relevant chunks are used along with the question to query the OpenAI's GPT-3.5-turbo model through Langchain's `ChatOpenAI` class.
* Finally, the GPT-3 model generates an answer based on the combined context.

### Cost Estimation:

* The `calculate_embedding_cost` function estimates the cost associated with generating embeddings for the document chunks based on the OpenAI API pricing.

### Streamlit User Interface

The application utilizes Streamlit for a user-friendly interface. Users can upload documents, adjust chunk size and the number of retrieved chunks (k), and then ask questions about the document. The application displays the answer generated by the GPT-3 model and maintains a chat history of questions and answers.

### How to run: `python -m streamlit run your_script.py` in your terminal after installing all dependencies.
